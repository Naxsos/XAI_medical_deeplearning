{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-ray Classification and Localization using DenseNet\n",
    "\n",
    "This notebook implements a transfer learning approach using DenseNet for chest X-ray classification and localization on the NIH Chest X-ray dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.11/site-packages (from torch) (4.13.0)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m181.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m150.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m185.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m182.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m194.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m196.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m238.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m250.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m243.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m218.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m223.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m132.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m163.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m222.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, pytz, nvidia-cusparselt-cu12, mpmath, tzdata, tqdm, threadpoolctl, sympy, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, fsspec, fonttools, filelock, cycler, scipy, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision\n",
      "Successfully installed MarkupSafe-3.0.2 contourpy-1.3.1 cycler-0.12.1 filelock-3.18.0 fonttools-4.56.0 fsspec-2025.3.0 jinja2-3.1.6 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.1 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 pillow-11.1.0 pyparsing-3.2.3 pytz-2025.2 scikit-learn-1.6.1 scipy-1.15.2 sympy-1.13.1 threadpoolctl-3.6.0 torch-2.6.0 torchvision-0.21.0 tqdm-4.67.1 triton-3.2.0 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision pandas numpy matplotlib tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed=10):\n",
    "    import os, random, numpy as np, torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH_RESIZED = os.path.join('resized_images_20k')\n",
    "PREPROCESSED_IMAGES_PATH = os.path.join('bbox_resized_filtered_images_20k_labled.csv')\n",
    "df_preprocessed = pd.read_csv(PREPROCESSED_IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, image_dir, df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['Image Index']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Get labels (binary classification: normal vs abnormal)\n",
    "        label = 1 if self.df.iloc[idx]['Finding Label'] != 'No Finding' else 0\n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return > 0\n",
    "print(torch.cuda.get_device_name(0))  # Should return the name of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    \"\"\"Define image transformations.\"\"\"\n",
    "    return {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=(-10, 10)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(10)\n",
    "\n",
    "# Split data and create data loaders\n",
    "train_df, temp_df = train_test_split(df_preprocessed, test_size=0.3, random_state=10)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=10)\n",
    "\n",
    "# Get transforms\n",
    "transforms_dict = get_transforms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 13839\n",
      "Validation samples: 2966\n",
      "Test samples: 2966\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(IMAGES_PATH_RESIZED, train_df, transform=transforms_dict['train'])\n",
    "val_dataset = ChestXrayDataset(IMAGES_PATH_RESIZED, val_df, transform=transforms_dict['val'])\n",
    "test_dataset = ChestXrayDataset(IMAGES_PATH_RESIZED, test_df, transform=transforms_dict['val'])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_map = self.sigmoid(self.conv(x))  # Generate attention map\n",
    "        return x * attn_map, attn_map\n",
    "\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):  # Changed default to 14 for NIH dataset\n",
    "        super().__init__()\n",
    "        # Load pre-trained ResNet18 instead of DenseNet\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the final fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # Unfreeze more layers for fine-tuning\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze the last two blocks\n",
    "        for param in list(self.features.children())[-3].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in list(self.features.children())[-2].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.attention = SpatialAttention(2048)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Simpler classifier with less dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes),\n",
    "            nn.Sigmoid()  # Keep sigmoid for multi-label\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x, attn_map = self.attention(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits, attn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMemory:\n",
    "    def __init__(self, memory_size=5, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            memory_size: Number of previous epochs to consider\n",
    "            alpha: Exponential moving average factor (higher = more weight to recent)\n",
    "        \"\"\"\n",
    "        #Keep the last 5 attention maps per image\n",
    "        self.memory_size = memory_size\n",
    "        #Controls how much more we trust recent maps (lower = longer memory)\n",
    "        self.alpha = alpha\n",
    "        self.attention_history = {}  # Maps image_id -> list of attention maps\n",
    "        self.correct_predictions = {}  # Maps image_id -> list of correctness flags\n",
    "    \n",
    "    def update(self, image_ids, attention_maps, predictions, labels):\n",
    "        \"\"\"Update memory with new attention maps\"\"\"\n",
    "        for i, img_id in enumerate(image_ids):\n",
    "            # Get current attention map and whether prediction was correct\n",
    "            attn = attention_maps[i].detach().cpu()\n",
    "            correct = (predictions[i].round() == labels[i]).all().float()\n",
    "            \n",
    "            # Initialize if first time seeing this image\n",
    "            if img_id not in self.attention_history:\n",
    "                self.attention_history[img_id] = []\n",
    "                self.correct_predictions[img_id] = []\n",
    "            \n",
    "            # Add new data\n",
    "            self.attention_history[img_id].append(attn)\n",
    "            self.correct_predictions[img_id].append(correct.item())\n",
    "            \n",
    "            # Keep only most recent entries\n",
    "            if len(self.attention_history[img_id]) > self.memory_size:\n",
    "                self.attention_history[img_id].pop(0)\n",
    "                self.correct_predictions[img_id].pop(0)\n",
    "    \n",
    "    def get_historical_attention(self, image_ids):\n",
    "        \"\"\"Get exponentially weighted historical attention maps\"\"\"\n",
    "        batch_history = []\n",
    "        \n",
    "        for img_id in image_ids:\n",
    "            if img_id not in self.attention_history or not self.attention_history[img_id]:\n",
    "                # No history available\n",
    "                batch_history.append(None)\n",
    "                continue\n",
    "            \n",
    "            # Get history for this image\n",
    "            history = self.attention_history[img_id]\n",
    "            correctness = self.correct_predictions[img_id]\n",
    "            \n",
    "            if len(history) == 1:\n",
    "                # Only one entry in history\n",
    "                batch_history.append(history[0])\n",
    "                continue\n",
    "            \n",
    "            # Calculate weighted average, giving more weight to:\n",
    "            # 1. More recent attention maps\n",
    "            # 2. Attention maps from correct predictions\n",
    "            weights = []\n",
    "            for i, is_correct in enumerate(correctness):\n",
    "                # Position weight (more recent = higher weight)\n",
    "                pos_weight = self.alpha ** (len(correctness) - i - 1)\n",
    "                # Correctness weight (correct predictions get higher weight)\n",
    "                correct_weight = 1.2 if is_correct else 0.8\n",
    "                weights.append(pos_weight * correct_weight)\n",
    "            \n",
    "            # Normalize weights\n",
    "            weights = [w / sum(weights) for w in weights]\n",
    "            \n",
    "            # Calculate weighted attention\n",
    "            weighted_attn = torch.zeros_like(history[0])\n",
    "            for i, attn in enumerate(history):\n",
    "                weighted_attn += weights[i] * attn\n",
    "            \n",
    "            batch_history.append(weighted_attn)\n",
    "        \n",
    "        return batch_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfCorrectiveAttentionLoss(nn.Module):\n",
    "    def __init__(self, lambda_consistency, lambda_sparsity):\n",
    "        super(SelfCorrectiveAttentionLoss, self).__init__()\n",
    "        self.cls_loss = nn.BCELoss()  # Keep BCE for multi-label\n",
    "        self.lambda_consistency = lambda_consistency\n",
    "        self.lambda_sparsity = lambda_sparsity\n",
    "    \n",
    "    def forward(self, pred, target, attn_map, historical_attn=None):\n",
    "        # Classification loss (primary objective)\n",
    "        cls_loss = self.cls_loss(pred, target)\n",
    "        \n",
    "        # Initialize attention losses\n",
    "        consistency_loss = torch.tensor(0.0).to(pred.device)\n",
    "        sparsity_loss = torch.tensor(0.0).to(pred.device)\n",
    "        \n",
    "        # Simpler sparsity loss\n",
    "        attn_flat = attn_map.view(attn_map.size(0), -1)\n",
    "        sparsity_loss = -torch.mean(torch.sum(attn_flat * torch.log(attn_flat + 1e-8), dim=1)) / 1000\n",
    "        \n",
    "        # Consistency loss (if historical attention is available)\n",
    "        if historical_attn is not None:\n",
    "            valid_indices = [i for i, h in enumerate(historical_attn) if h is not None]\n",
    "            \n",
    "            if valid_indices:\n",
    "                valid_hist = torch.stack([historical_attn[i] for i in valid_indices]).to(pred.device)\n",
    "                valid_curr = attn_map[valid_indices]\n",
    "                \n",
    "                # For multi-label, consider prediction correct if all labels match\n",
    "                correct_mask = (pred.round() == target).all(dim=1).float()[valid_indices].view(-1, 1, 1)\n",
    "                incorrect_mask = 1 - correct_mask\n",
    "                \n",
    "                consistency_term = torch.abs(valid_curr - valid_hist) * correct_mask\n",
    "                correction_term = torch.exp(-torch.abs(valid_curr - valid_hist)) * incorrect_mask\n",
    "                \n",
    "                consistency_loss = torch.mean(consistency_term + correction_term)\n",
    "        \n",
    "        # Total loss with reduced weights\n",
    "        total_loss = cls_loss + self.lambda_consistency * consistency_loss + self.lambda_sparsity * sparsity_loss\n",
    "        \n",
    "        return total_loss, cls_loss, consistency_loss, sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_maps(model, data_loader, num_samples=5, save_dir='attention_maps'):\n",
    "    \"\"\"\n",
    "    Visualize attention maps for a few samples\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader for the dataset\n",
    "        num_samples: Number of samples to visualize\n",
    "        save_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a few samples\n",
    "    samples = []\n",
    "    for images, labels, img_ids in data_loader:\n",
    "        for i in range(min(len(images), num_samples - len(samples))):\n",
    "            samples.append((images[i], labels[i], img_ids[i]))\n",
    "        if len(samples) >= num_samples:\n",
    "            break\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (image, label, img_id) in enumerate(samples):\n",
    "            # Get model prediction and attention map\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            logit, attn_map = model(image)\n",
    "            pred = torch.sigmoid(logit).item()\n",
    "            \n",
    "            # Convert to numpy for visualization\n",
    "            image_np = image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "            # Denormalize image\n",
    "            image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            image_np = np.clip(image_np, 0, 1)\n",
    "            \n",
    "            # Get attention map\n",
    "            attn_map_np = attn_map.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Create visualization\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Original image\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f\"Original Image\\nTrue: {label.item()}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Attention map\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(attn_map_np, cmap='hot')\n",
    "            plt.title(f\"Attention Map\\nPred: {pred:.4f}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(image_np)\n",
    "            plt.imshow(attn_map_np, cmap='hot', alpha=0.5)\n",
    "            plt.title(\"Overlay\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_dir}/attention_{img_id.replace('.', '_')}.png\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_with_self_correction(model, train_loader, val_loader, num_epochs=15):\n",
    "    \"\"\"\n",
    "    Train the model with self-corrective attention \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Train the model with self-corrective attention \n",
    "    \"\"\"\n",
    "    # Initialize attention memory\n",
    "    memory = AttentionMemory(memory_size=3, alpha=0.7)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = SelfCorrectiveAttentionLoss(lambda_consistency=0, lambda_sparsity=0.001)\n",
    "\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': list(model.features.children())[-2].parameters(), 'lr': 1e-4},\n",
    "        {'params': model.attention.parameters(), 'lr': 2e-4},\n",
    "        {'params': model.classifier.parameters(), 'lr': 2e-4}\n",
    "    ], weight_decay=1e-4) \n",
    "\n",
    "    # Learning rate scheduler with more patience\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.7, patience=3, verbose=True\n",
    "    )\n",
    "        \n",
    "    # Track metrics\n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_auc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_auc': [],\n",
    "        'cls_loss': [], 'consistency_loss': [], 'sparsity_loss': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        epoch_cls_loss = 0.0\n",
    "        epoch_consistency_loss = 0.0\n",
    "        epoch_sparsity_loss = 0.0\n",
    "        \n",
    "        # For AUC calculation\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for images, labels, img_ids in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get historical attention maps for batch\n",
    "            historical_attn = memory.get_historical_attention(img_ids)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits, attn_maps = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, cls_loss, consistency_loss, sparsity_loss = criterion(\n",
    "                logits, labels, attn_maps, historical_attn\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            epoch_cls_loss += cls_loss.item()\n",
    "            epoch_consistency_loss += consistency_loss.item()\n",
    "            epoch_sparsity_loss += sparsity_loss.item()\n",
    "            \n",
    "            # Calculate accuracy (all labels must match)\n",
    "            preds = (logits > 0.5).float()\n",
    "            train_correct += (preds == labels).all(dim=1).sum().item()\n",
    "\n",
    "            # Store predictions and labels for AUC\n",
    "            train_preds.append(logits.detach().cpu().numpy())\n",
    "            train_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            # Update attention memory\n",
    "            memory.update(img_ids, attn_maps, logits, labels)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        # For AUC calculation\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, img_ids in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Get historical attention\n",
    "                historical_attn = memory.get_historical_attention(img_ids)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, attn_maps = model(images)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss, _, _, _ = criterion(logits, labels, attn_maps, historical_attn)\n",
    "                \n",
    "                # Track metrics\n",
    "                val_loss += loss.item()\n",
    "                preds = (logits > 0.5).float()\n",
    "                val_correct += (preds == labels).all(dim=1).sum().item()\n",
    "\n",
    "                # Store predictions and labels for AUC\n",
    "                val_preds.append(logits.cpu().numpy())\n",
    "                val_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_cls_loss = epoch_cls_loss / len(train_loader)\n",
    "        avg_consistency_loss = epoch_consistency_loss / len(train_loader)\n",
    "        avg_sparsity_loss = epoch_sparsity_loss / len(train_loader)\n",
    "        train_accuracy = 100 * train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # Convert lists to numpy arrays for AUC calculation\n",
    "        train_preds = np.concatenate(train_preds, axis=0)\n",
    "        train_labels = np.concatenate(train_labels, axis=0)\n",
    "        val_preds = np.concatenate(val_preds, axis=0)\n",
    "        val_labels = np.concatenate(val_labels, axis=0)\n",
    "        \n",
    "        # Calculate per-label AUC and average\n",
    "        train_aucs = [roc_auc_score(train_labels[:, i], train_preds[:, i]) \n",
    "                     for i in range(train_preds.shape[1])]\n",
    "        train_auc = np.mean(train_aucs)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        # Calculate per-label AUC and average for validation\n",
    "        val_aucs = [roc_auc_score(val_labels[:, i], val_preds[:, i]) \n",
    "                   for i in range(val_preds.shape[1])]\n",
    "        val_auc = np.mean(val_aucs)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        history['cls_loss'].append(avg_cls_loss)\n",
    "        history['consistency_loss'].append(avg_consistency_loss)\n",
    "        history['sparsity_loss'].append(avg_sparsity_loss)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Cls: {avg_cls_loss:.4f}, ' + \n",
    "              f'Consistency: {avg_consistency_loss:.4f}, Sparsity: {avg_sparsity_loss:.4f})')\n",
    "        print(f'Train Accuracy: {train_accuracy:.2f}%, AUC: {train_auc:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, AUC: {val_auc:.4f}')\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_auc)  # Use AUC for scheduling\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_self_corrective.pth')\n",
    "            print('Model saved!')\n",
    "        \n",
    "        print('-' * 60)\n",
    "\n",
    "        #if epoch % 2 == 0:  # Save every 2 epochs to reduce storage\n",
    "        #    print(\"Visualizing attention maps...\")\n",
    "        #    visualize_attention_maps(model, train_loader, \n",
    "        #                            save_dir=f'attention_maps/epoch_{epoch+1}/train')\n",
    "        #    visualize_attention_maps(model, val_loader, \n",
    "        #                            save_dir=f'attention_maps/epoch_{epoch+1}/val')\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Accuracy curves\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: AUC curves\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['train_auc'], label='Train AUC')\n",
    "    plt.plot(history['val_auc'], label='Validation AUC')\n",
    "    plt.title('AUC Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 4: Component losses\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history['cls_loss'], label='Classification Loss')\n",
    "    plt.plot(history['consistency_loss'], label='Consistency Loss')\n",
    "    plt.plot(history['sparsity_loss'], label='Sparsity Loss')\n",
    "    plt.title('Component Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 13839\n",
      "Validation samples: 2966\n",
      "Test samples: 2966\n",
      "Number of classes: 10\n",
      "Classes: ['Atelectasis' 'Cardiomegaly' 'Effusion' 'Infiltrate' 'Infiltration'\n",
      " 'Mass' 'No Finding' 'Nodule' 'Pneumonia' 'Pneumothorax']\n"
     ]
    }
   ],
   "source": [
    "class ChestXrayDatasetWithIDs(Dataset):\n",
    "    def __init__(self, image_dir, df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Create label encoder for the Finding Label column\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(df['Finding Label'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['Image Index']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Get label and convert to one-hot encoding\n",
    "        label = self.df.iloc[idx]['Finding Label']\n",
    "        label_idx = self.label_encoder.transform([label])[0]\n",
    "        num_classes = len(self.label_encoder.classes_)\n",
    "        label_onehot = torch.zeros(num_classes, dtype=torch.float32)\n",
    "        label_onehot[label_idx] = 1.0\n",
    "        \n",
    "        # Return image ID along with image and labels\n",
    "        return image, label_onehot, img_name\n",
    "\n",
    "# Create datasets with image IDs\n",
    "train_dataset = ChestXrayDatasetWithIDs(IMAGES_PATH_RESIZED, train_df, transform=transforms_dict['train'])\n",
    "val_dataset = ChestXrayDatasetWithIDs(IMAGES_PATH_RESIZED, val_df, transform=transforms_dict['val'])\n",
    "test_dataset = ChestXrayDatasetWithIDs(IMAGES_PATH_RESIZED, test_df, transform=transforms_dict['val'])\n",
    "\n",
    "batch_size = 32\n",
    "# Create data loaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(10)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {len(train_dataset.label_encoder.classes_)}\")\n",
    "print(\"Classes:\", train_dataset.label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/XAI_medical_deeplearning/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/XAI_medical_deeplearning/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/root/XAI_medical_deeplearning/myenv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/10: 100%|██████████| 433/433 [00:29<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 0.2420 (Cls: 0.2420, Consistency: 0.0000, Sparsity: 0.0104)\n",
      "Train Accuracy: 36.19%, AUC: 0.5972\n",
      "Validation Loss: 0.1887, Accuracy: 42.28%, AUC: 0.7111\n",
      "Model saved!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 433/433 [00:29<00:00, 14.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "Train Loss: 0.1892 (Cls: 0.1892, Consistency: 0.5533, Sparsity: 0.0068)\n",
      "Train Accuracy: 43.75%, AUC: 0.6999\n",
      "Validation Loss: 0.1887, Accuracy: 43.46%, AUC: 0.7388\n",
      "Model saved!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 433/433 [00:29<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "Train Loss: 0.1819 (Cls: 0.1819, Consistency: 0.5359, Sparsity: 0.0058)\n",
      "Train Accuracy: 46.00%, AUC: 0.7552\n",
      "Validation Loss: 0.1817, Accuracy: 47.44%, AUC: 0.7721\n",
      "Model saved!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 433/433 [00:30<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "Train Loss: 0.1789 (Cls: 0.1789, Consistency: 0.5313, Sparsity: 0.0056)\n",
      "Train Accuracy: 46.53%, AUC: 0.7724\n",
      "Validation Loss: 0.1856, Accuracy: 43.80%, AUC: 0.7317\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 433/433 [00:29<00:00, 14.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "Train Loss: 0.1770 (Cls: 0.1770, Consistency: 0.5188, Sparsity: 0.0065)\n",
      "Train Accuracy: 48.19%, AUC: 0.7884\n",
      "Validation Loss: 0.1803, Accuracy: 50.10%, AUC: 0.7767\n",
      "Model saved!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 433/433 [00:30<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "Train Loss: 0.1738 (Cls: 0.1738, Consistency: 0.5085, Sparsity: 0.0048)\n",
      "Train Accuracy: 49.33%, AUC: 0.7950\n",
      "Validation Loss: 0.1856, Accuracy: 44.00%, AUC: 0.7580\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 433/433 [00:30<00:00, 14.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "num_classes = len(train_dataset.label_encoder.classes_)\n",
    "model = ResNetWithAttention(num_classes=num_classes).to(device)\n",
    "\n",
    "# model = ResNetWithAttention().to(device)\n",
    "\n",
    "# Train with self-correction\n",
    "trained_model, history = train_with_self_correction(model, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = AttentionMemory(memory_size=5, alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
