{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-ray Classification and Localization using DenseNet\n",
    "\n",
    "This notebook implements a transfer learning approach using DenseNet for chest X-ray classification and localization on the NIH Chest X-ray dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m254.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m223.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m169.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m152.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m224.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m271.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m283.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m257.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m255.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m284.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m281.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m247.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m209.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m180.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m148.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m245.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m179.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, pytz, nvidia-cusparselt-cu12, mpmath, tzdata, tqdm, sympy, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, fsspec, fonttools, filelock, cycler, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, contourpy, nvidia-cusolver-cu12, matplotlib, torch, torchvision\n",
      "Successfully installed MarkupSafe-3.0.2 contourpy-1.3.1 cycler-0.12.1 filelock-3.18.0 fonttools-4.56.0 fsspec-2025.3.0 jinja2-3.1.6 kiwisolver-1.4.8 matplotlib-3.10.1 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 pillow-11.1.0 pyparsing-3.2.1 pytz-2025.1 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 tqdm-4.67.1 triton-3.2.0 tzdata-2025.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision pandas numpy matplotlib tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH_RESIZED = os.path.join('resized_images_2')\n",
    "PREPROCESSED_IMAGES_PATH = os.path.join('bbox_resized_filtered_images_10k.csv')\n",
    "df_preprocessed = pd.read_csv(PREPROCESSED_IMAGES_PATH)\n",
    "df_preprocessed = df_preprocessed.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, image_dir, df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['Image Index']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Get labels (binary classification: normal vs abnormal)\n",
    "        label = 1 if self.df.iloc[idx]['Finding Label'] != 'No Finding' else 0\n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return > 0\n",
    "print(torch.cuda.get_device_name(0))  # Should return the name of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    \"\"\"Define image transformations.\"\"\"\n",
    "    return {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(10)\n",
    "\n",
    "# Split data and create data loaders\n",
    "train_df, temp_df = train_test_split(df_preprocessed, test_size=0.2, random_state=10)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=10)\n",
    "\n",
    "# Get transforms\n",
    "transforms_dict = get_transforms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7200\n",
      "Validation samples: 900\n",
      "Test samples: 900\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(IMAGES_PATH_RESIZED, train_df, transform=transforms_dict['train'])\n",
    "val_dataset = ChestXrayDataset(IMAGES_PATH_RESIZED, val_df, transform=transforms_dict['val'])\n",
    "test_dataset = ChestXrayDataset(IMAGES_PATH_RESIZED, test_df, transform=transforms_dict['val'])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, generator=g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_map = self.sigmoid(self.conv(x))  # Generate attention map\n",
    "        return x * attn_map, attn_map\n",
    "\n",
    "class DenseNetWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        # Load pre-trained DenseNet but freeze more layers\n",
    "        densenet = models.densenet121(pretrained=True)\n",
    "        self.features = densenet.features\n",
    "        \n",
    "        # 1) Freeze everything first\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last dense block (denseblock4) and norm5\n",
    "        for name, child in self.features.named_children():\n",
    "            if name == 'denseblock4' or name == 'norm5':\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        self.attention = SpatialAttention(1024)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Simpler classifier with stronger regularization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),  # Added batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # Increased dropout\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First run feature extraction\n",
    "        x = self.features(x)\n",
    "        # Then apply attention to features\n",
    "        x, attn_map = self.attention(x)\n",
    "        # Pool and classify\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits, attn_map\n",
    "\n",
    "# Create model\n",
    "#model = DenseNetWithAttention().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMemory:\n",
    "    def __init__(self, memory_size=5, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            memory_size: Number of previous epochs to consider\n",
    "            alpha: Exponential moving average factor (higher = more weight to recent)\n",
    "        \"\"\"\n",
    "        #Keep the last 5 attention maps per image\n",
    "        self.memory_size = memory_size\n",
    "        #Controls how much more we trust recent maps (lower = longer memory)\n",
    "        self.alpha = alpha\n",
    "        self.attention_history = {}  # Maps image_id -> list of attention maps\n",
    "        self.correct_predictions = {}  # Maps image_id -> list of correctness flags\n",
    "    \n",
    "    def update(self, image_ids, attention_maps, predictions, labels):\n",
    "        \"\"\"Update memory with new attention maps\"\"\"\n",
    "        for i, img_id in enumerate(image_ids):\n",
    "            # Get current attention map and whether prediction was correct\n",
    "            attn = attention_maps[i].detach().cpu()\n",
    "            correct = (predictions[i] > 0.5).float() == labels[i]\n",
    "            \n",
    "            # Initialize if first time seeing this image\n",
    "            if img_id not in self.attention_history:\n",
    "                self.attention_history[img_id] = []\n",
    "                self.correct_predictions[img_id] = []\n",
    "            \n",
    "            # Add new data\n",
    "            self.attention_history[img_id].append(attn)\n",
    "            self.correct_predictions[img_id].append(correct.item())\n",
    "            \n",
    "            # Keep only most recent entries\n",
    "            if len(self.attention_history[img_id]) > self.memory_size:\n",
    "                self.attention_history[img_id].pop(0)\n",
    "                self.correct_predictions[img_id].pop(0)\n",
    "    \n",
    "    def get_historical_attention(self, image_ids):\n",
    "        \"\"\"Get exponentially weighted historical attention maps\"\"\"\n",
    "        batch_history = []\n",
    "        \n",
    "        for img_id in image_ids:\n",
    "            if img_id not in self.attention_history or not self.attention_history[img_id]:\n",
    "                # No history available\n",
    "                batch_history.append(None)\n",
    "                continue\n",
    "            \n",
    "            # Get history for this image\n",
    "            history = self.attention_history[img_id]\n",
    "            correctness = self.correct_predictions[img_id]\n",
    "            \n",
    "            if len(history) == 1:\n",
    "                # Only one entry in history\n",
    "                batch_history.append(history[0])\n",
    "                continue\n",
    "            \n",
    "            # Calculate weighted average, giving more weight to:\n",
    "            # 1. More recent attention maps\n",
    "            # 2. Attention maps from correct predictions\n",
    "            weights = []\n",
    "            for i, is_correct in enumerate(correctness):\n",
    "                # Position weight (more recent = higher weight)\n",
    "                pos_weight = self.alpha ** (len(correctness) - i - 1)\n",
    "                # Correctness weight (correct predictions get higher weight)\n",
    "                correct_weight = 1.2 if is_correct else 0.8\n",
    "                weights.append(pos_weight * correct_weight)\n",
    "            \n",
    "            # Normalize weights\n",
    "            weights = [w / sum(weights) for w in weights]\n",
    "            \n",
    "            # Calculate weighted attention\n",
    "            weighted_attn = torch.zeros_like(history[0])\n",
    "            for i, attn in enumerate(history):\n",
    "                weighted_attn += weights[i] * attn\n",
    "            \n",
    "            batch_history.append(weighted_attn)\n",
    "        \n",
    "        return batch_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfCorrectiveAttentionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Tell model how to correct its attention behaviour\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_consistency=0.2, lambda_sparsity=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lambda_consistency: Weight for consistency term\n",
    "            lambda_sparsity: Weight for sparsity term\n",
    "        \"\"\"\n",
    "        super(SelfCorrectiveAttentionLoss, self).__init__()\n",
    "        self.cls_loss = nn.BCELoss() ## here maybe BCEWithLogitsLoss?\n",
    "        self.lambda_consistency = lambda_consistency\n",
    "        self.lambda_sparsity = lambda_sparsity\n",
    "    \n",
    "    def forward(self, pred, target, attn_map, historical_attn=None):\n",
    "\n",
    "        \"\"\"\n",
    "            pred: model predictions\n",
    "            target: ground truth labels\n",
    "            attn_map: current attention map\n",
    "            historical_attn: historical attention maps\n",
    "        \"\"\"\n",
    "        # Classification loss (primary objective)\n",
    "        cls_loss = self.cls_loss(pred, target)\n",
    "        \n",
    "        # Initialize attention losses\n",
    "        consistency_loss = torch.tensor(0.0).to(pred.device)\n",
    "        sparsity_loss = torch.tensor(0.0).to(pred.device)\n",
    "        \n",
    "        # Attention sparsity loss (encourage focused attention)\n",
    "        # attention should be concentrated, not diffused\n",
    "        attn_flat = attn_map.view(attn_map.size(0), -1)\n",
    "        sparsity_loss = -torch.mean(torch.sum(attn_flat * torch.log(attn_flat + 1e-8), dim=1)) / 100\n",
    "        \n",
    "        # Consistency loss (if historical attention is available)\n",
    "        if historical_attn is not None:\n",
    "            valid_indices = [i for i, h in enumerate(historical_attn) if h is not None]\n",
    "            \n",
    "            if valid_indices:\n",
    "                # Stack valid historical attention maps\n",
    "                valid_hist = torch.stack([historical_attn[i] for i in valid_indices]).to(pred.device)\n",
    "                valid_curr = attn_map[valid_indices]\n",
    "                \n",
    "                # Calculate consistency loss\n",
    "                # For correct predictions: encourage consistency with history\n",
    "                # For incorrect predictions: encourage deviation from history\n",
    "                correct_mask = (pred.round() == target).float()[valid_indices].view(-1, 1, 1)\n",
    "                incorrect_mask = 1 - correct_mask\n",
    "                \n",
    "                # For correct predictions: minimize distance to historical attention\n",
    "                consistency_term = torch.abs(valid_curr - valid_hist) * correct_mask\n",
    "                \n",
    "                # For incorrect predictions: maximize distance to historical attention\n",
    "                # But only if historical prediction was also wrong\n",
    "                correction_term = torch.exp(-torch.abs(valid_curr - valid_hist)) * incorrect_mask\n",
    "                \n",
    "                consistency_loss = torch.mean(consistency_term + correction_term)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = cls_loss + self.lambda_consistency * consistency_loss + self.lambda_sparsity * sparsity_loss #(comment out for baseline classification loss)\n",
    "        #total_loss = cls_loss\n",
    "        \n",
    "        return total_loss, cls_loss, consistency_loss, sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_maps(model, data_loader, num_samples=5, save_dir='attention_maps'):\n",
    "    \"\"\"\n",
    "    Visualize attention maps for a few samples\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: DataLoader for the dataset\n",
    "        num_samples: Number of samples to visualize\n",
    "        save_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a few samples\n",
    "    samples = []\n",
    "    for images, labels, img_ids in data_loader:\n",
    "        for i in range(min(len(images), num_samples - len(samples))):\n",
    "            samples.append((images[i], labels[i], img_ids[i]))\n",
    "        if len(samples) >= num_samples:\n",
    "            break\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (image, label, img_id) in enumerate(samples):\n",
    "            # Get model prediction and attention map\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            logit, attn_map = model(image)\n",
    "            pred = torch.sigmoid(logit).item()\n",
    "            \n",
    "            # Convert to numpy for visualization\n",
    "            image_np = image.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "            # Denormalize image\n",
    "            image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            image_np = np.clip(image_np, 0, 1)\n",
    "            \n",
    "            # Get attention map\n",
    "            attn_map_np = attn_map.squeeze().cpu().numpy()\n",
    "            \n",
    "            # Create visualization\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Original image\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(image_np)\n",
    "            plt.title(f\"Original Image\\nTrue: {label.item()}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Attention map\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(attn_map_np, cmap='hot')\n",
    "            plt.title(f\"Attention Map\\nPred: {pred:.4f}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(image_np)\n",
    "            plt.imshow(attn_map_np, cmap='hot', alpha=0.5)\n",
    "            plt.title(\"Overlay\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{save_dir}/attention_{img_id.replace('.', '_')}.png\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "def train_with_self_correction(model, train_loader, val_loader, num_epochs=15):\n",
    "    \"\"\"\n",
    "    Train the model with self-corrective attention \n",
    "    \"\"\"\n",
    "    # Initialize attention memory\n",
    "    memory = AttentionMemory(memory_size=3, alpha=0.7)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = SelfCorrectiveAttentionLoss(lambda_consistency=0.001, lambda_sparsity=0.001)\n",
    "\n",
    "    #optimizer = optim.AdamW([\n",
    "    #    #{'params': model.features.denseblock4.parameters(), 'lr': 5e-5},\n",
    "    #    {'params': model.features.norm5.parameters(), 'lr': 5e-5},\n",
    "    #    {'params': model.attention.parameters(), 'lr': 1e-4},\n",
    "    #    {'params': model.classifier.parameters(), 'lr': 5e-3}\n",
    "    #], weight_decay=1e-4)  # Increased weight decay\n",
    "\n",
    "    #optimizer = optim.SGD([\n",
    "    #    {'params': model.features.norm5.parameters(), 'lr': 1e-4},\n",
    "    #    {'params': model.attention.parameters(), 'lr': 5e-4},\n",
    "    #    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "    #], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "    # Use different learning rates for different parts of the model\n",
    "    base_params = [p for n, p in model.named_parameters() if 'features' not in n]\n",
    "    feature_params = [p for n, p in model.named_parameters() if 'features' in n and p.requires_grad]\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': base_params},\n",
    "        {'params': feature_params, 'lr': 1e-5}  # Lower learning rate for pre-trained layers\n",
    "    ], lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "        \n",
    "    # Track metrics\n",
    "    best_val_loss = float('inf')\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_auc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_auc': [],\n",
    "        'cls_loss': [], 'consistency_loss': [], 'sparsity_loss': []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        epoch_cls_loss = 0.0\n",
    "        epoch_consistency_loss = 0.0\n",
    "        epoch_sparsity_loss = 0.0\n",
    "        \n",
    "        # For AUC calculation\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for images, labels, img_ids in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get historical attention maps for batch\n",
    "            historical_attn = memory.get_historical_attention(img_ids)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits, attn_maps = model(images)\n",
    "            logits = logits.squeeze()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, cls_loss, consistency_loss, sparsity_loss = criterion(\n",
    "                logits, labels, attn_maps, historical_attn\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            epoch_cls_loss += cls_loss.item()\n",
    "            epoch_consistency_loss += consistency_loss.item()\n",
    "            epoch_sparsity_loss += sparsity_loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = (logits > 0.5).float()\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "\n",
    "            # Store predictions and labels for AUC\n",
    "            train_preds.extend(logits.detach().cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update attention memory\n",
    "            memory.update(img_ids, attn_maps, logits, labels)\n",
    "\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        # For AUC calculation\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, img_ids in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Get historical attention\n",
    "                historical_attn = memory.get_historical_attention(img_ids)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, attn_maps = model(images)\n",
    "                logits = logits.squeeze()\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss, _, _, _ = criterion(logits, labels, attn_maps, historical_attn)\n",
    "                \n",
    "                # Track metrics\n",
    "                val_loss += loss.item()\n",
    "                preds = (logits > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "\n",
    "                # Store predictions and labels for AUC\n",
    "                val_preds.extend(logits.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Update memory even for validation (helps track consistency)\n",
    "                #memory.update(img_ids, attn_maps, logits, labels)\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_cls_loss = epoch_cls_loss / len(train_loader)\n",
    "        avg_consistency_loss = epoch_consistency_loss / len(train_loader)\n",
    "        avg_sparsity_loss = epoch_sparsity_loss / len(train_loader)\n",
    "        train_accuracy = 100 * train_correct / len(train_loader.dataset)\n",
    "        train_auc = roc_auc_score(train_labels, train_preds)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / len(val_loader.dataset)\n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        history['cls_loss'].append(avg_cls_loss)\n",
    "        history['consistency_loss'].append(avg_consistency_loss)\n",
    "        history['sparsity_loss'].append(avg_sparsity_loss)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f} (Cls: {avg_cls_loss:.4f}, ' + \n",
    "              f'Consistency: {avg_consistency_loss:.4f}, Sparsity: {avg_sparsity_loss:.4f})')\n",
    "        print(f'Train Accuracy: {train_accuracy:.2f}%, AUC: {train_auc:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, AUC: {val_auc:.4f}')\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_self_corrective.pth')\n",
    "            print('Model saved!')\n",
    "        \n",
    "        print('-' * 60)\n",
    "\n",
    "        #if epoch % 2 == 0:  # Save every 2 epochs to reduce storage\n",
    "        #    print(\"Visualizing attention maps...\")\n",
    "        #    visualize_attention_maps(model, train_loader, \n",
    "        #                            save_dir=f'attention_maps/epoch_{epoch+1}/train')\n",
    "        #    visualize_attention_maps(model, val_loader, \n",
    "        #                            save_dir=f'attention_maps/epoch_{epoch+1}/val')\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Accuracy curves\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(50, 100)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 3: AUC curves\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['train_auc'], label='Train AUC')\n",
    "    plt.plot(history['val_auc'], label='Validation AUC')\n",
    "    plt.title('AUC Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.ylim(0.5, 1)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 4: Component losses\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history['cls_loss'], label='Classification Loss')\n",
    "    plt.plot(history['consistency_loss'], label='Consistency Loss')\n",
    "    plt.plot(history['sparsity_loss'], label='Sparsity Loss')\n",
    "    plt.title('Component Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7200\n",
      "Validation samples: 900\n",
      "Test samples: 900\n"
     ]
    }
   ],
   "source": [
    "class ChestXrayDatasetWithIDs(Dataset):\n",
    "    def __init__(self, image_dir, df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['Image Index']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Get labels (binary classification: normal vs abnormal)\n",
    "        label = 1 if self.df.iloc[idx]['Finding Label'] != 'No Finding' else 0\n",
    "        \n",
    "        # Return image ID along with image and label\n",
    "        return image, torch.tensor(label, dtype=torch.float32), img_name\n",
    "\n",
    "# Create datasets with image IDs\n",
    "train_dataset = ChestXrayDatasetWithIDs(IMAGES_PATH_RESIZED, train_df, transform=transforms_dict['train'])\n",
    "val_dataset = ChestXrayDatasetWithIDs(IMAGES_PATH_RESIZED, val_df, transform=transforms_dict['val'])\n",
    "test_dataset = ChestXrayDatasetWithIDs(IMAGES_PATH_RESIZED, test_df, transform=transforms_dict['val'])\n",
    "\n",
    "batch_size = 32\n",
    "# Create data loaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(10)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, generator=g, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/XAI_medical_deeplearning/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/XAI_medical_deeplearning/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/root/XAI_medical_deeplearning/myenv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/15: 100%|██████████| 225/225 [00:13<00:00, 17.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:\n",
      "Train Loss: 0.5391 (Cls: 0.5189, Consistency: 0.0516, Sparsity: 0.1500)\n",
      "Train Accuracy: 75.82%, AUC: 0.6755\n",
      "Validation Loss: 0.5055, Accuracy: 78.89%, AUC: 0.7334\n",
      "Model saved!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 225/225 [00:14<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15:\n",
      "Train Loss: 0.4990 (Cls: 0.4557, Consistency: 0.2950, Sparsity: 0.1382)\n",
      "Train Accuracy: 79.07%, AUC: 0.7644\n",
      "Validation Loss: 0.4824, Accuracy: 79.89%, AUC: 0.7493\n",
      "Model saved!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 225/225 [00:13<00:00, 17.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15:\n",
      "Train Loss: 0.4712 (Cls: 0.4308, Consistency: 0.2738, Sparsity: 0.1303)\n",
      "Train Accuracy: 80.61%, AUC: 0.7981\n",
      "Validation Loss: 0.4774, Accuracy: 80.11%, AUC: 0.7583\n",
      "Model saved!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 225/225 [00:15<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15:\n",
      "Train Loss: 0.4460 (Cls: 0.4073, Consistency: 0.2586, Sparsity: 0.1277)\n",
      "Train Accuracy: 81.69%, AUC: 0.8247\n",
      "Validation Loss: 0.4720, Accuracy: 79.56%, AUC: 0.7718\n",
      "Model saved!\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 225/225 [00:15<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15:\n",
      "Train Loss: 0.4212 (Cls: 0.3838, Consistency: 0.2519, Sparsity: 0.1214)\n",
      "Train Accuracy: 82.33%, AUC: 0.8483\n",
      "Validation Loss: 0.4885, Accuracy: 77.89%, AUC: 0.7612\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 225/225 [00:15<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15:\n",
      "Train Loss: 0.3831 (Cls: 0.3485, Consistency: 0.2312, Sparsity: 0.1149)\n",
      "Train Accuracy: 84.81%, AUC: 0.8830\n",
      "Validation Loss: 0.4786, Accuracy: 79.00%, AUC: 0.7771\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 225/225 [00:15<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15:\n",
      "Train Loss: 0.3652 (Cls: 0.3321, Consistency: 0.2177, Sparsity: 0.1136)\n",
      "Train Accuracy: 85.69%, AUC: 0.8940\n",
      "Validation Loss: 0.4897, Accuracy: 78.56%, AUC: 0.7766\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 225/225 [00:15<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15:\n",
      "Train Loss: 0.3468 (Cls: 0.3146, Consistency: 0.2121, Sparsity: 0.1099)\n",
      "Train Accuracy: 86.19%, AUC: 0.9069\n",
      "Validation Loss: 0.4843, Accuracy: 79.22%, AUC: 0.7778\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 225/225 [00:15<00:00, 14.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15:\n",
      "Train Loss: 0.3280 (Cls: 0.2969, Consistency: 0.2034, Sparsity: 0.1077)\n",
      "Train Accuracy: 87.28%, AUC: 0.9185\n",
      "Validation Loss: 0.5145, Accuracy: 79.44%, AUC: 0.7653\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 225/225 [00:14<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15:\n",
      "Train Loss: 0.3117 (Cls: 0.2822, Consistency: 0.1924, Sparsity: 0.1022)\n",
      "Train Accuracy: 88.60%, AUC: 0.9266\n",
      "Validation Loss: 0.5357, Accuracy: 78.67%, AUC: 0.7632\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 225/225 [00:15<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15:\n",
      "Train Loss: 0.2948 (Cls: 0.2660, Consistency: 0.1882, Sparsity: 0.0991)\n",
      "Train Accuracy: 88.97%, AUC: 0.9359\n",
      "Validation Loss: 0.5401, Accuracy: 77.78%, AUC: 0.7653\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 225/225 [00:14<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15:\n",
      "Train Loss: 0.2775 (Cls: 0.2497, Consistency: 0.1830, Sparsity: 0.0957)\n",
      "Train Accuracy: 89.60%, AUC: 0.9450\n",
      "Validation Loss: 0.5844, Accuracy: 77.89%, AUC: 0.7517\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 225/225 [00:14<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15:\n",
      "Train Loss: 0.2649 (Cls: 0.2378, Consistency: 0.1774, Sparsity: 0.0936)\n",
      "Train Accuracy: 90.21%, AUC: 0.9502\n",
      "Validation Loss: 0.5751, Accuracy: 79.00%, AUC: 0.7635\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 225/225 [00:13<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15:\n",
      "Train Loss: 0.2492 (Cls: 0.2234, Consistency: 0.1691, Sparsity: 0.0889)\n",
      "Train Accuracy: 91.11%, AUC: 0.9559\n",
      "Validation Loss: 0.5872, Accuracy: 78.33%, AUC: 0.7618\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 225/225 [00:14<00:00, 15.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15:\n",
      "Train Loss: 0.2377 (Cls: 0.2125, Consistency: 0.1644, Sparsity: 0.0871)\n",
      "Train Accuracy: 91.62%, AUC: 0.9607\n",
      "Validation Loss: 0.6212, Accuracy: 78.89%, AUC: 0.7569\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = DenseNetWithAttention().to(device)\n",
    "\n",
    "# Train with self-correction\n",
    "trained_model, history = train_with_self_correction(model, train_loader, val_loader, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = AttentionMemory(memory_size=5, alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
